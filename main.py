# Função de ativação (degrau ajustado)
def ativacao(x):
    return 1 if x >= 0 else -1


# Função para converter número em vetor de dígitos
def numeros_para_digitos(number):
    return [int(digit) for digit in str(number).zfill(8)]


# Treinamento do Perceptron
def treino_perceptron(dados, taxa_aprendizagem, bias):
    pesos = [1] * 8  # Inicializando os pesos com 1
    iteracao_atual = 0

    while True:
        total_erro = 0
        total_erro_quadrado = 0
        deltas_acumulados = [0] * 8
        iteracao_atual += 1

        for number in dados:
            digits = numeros_para_digitos(number)
            saida_desejada= 1 if number >= ru_number else -1

            soma_ponderada = sum(w * d for w, d in zip(pesos, digits)) + bias
            saida = ativacao(soma_ponderada)

            erro = saida_desejada- saida
            total_erro += erro
            total_erro_quadrado += erro ** 2

            deltas = [taxa_aprendizagem * erro * d for d in digits]
            deltas_acumulados = [da + delta for da, delta in zip(deltas_acumulados, deltas)]

        # Ajustar pesos no final de cada iteração
        media_delta = [da / len(dados) for da in deltas_acumulados]
        pesos = [w + md for w, md in zip(pesos, media_delta)]

        # Critério de parada
        if total_erro == 0 and total_erro_quadrado == 0:
            break

    return iteracao_atual, pesos, bias, total_erro, total_erro_quadrado


# Dados e parâmetros
ru_number = 3288581
numbers = [
    3288581, 1832197, 35497291, 89988642, 93412451, 63483921, 93627493, 41331864, 69294658, 33805865,
    25695216, 86413155, 57166353, 32058979, 97338847, 47577680, 55396862, 49497160, 48433790, 90844021,
    99422574, 70087216, 26488645, 19825183, 38327424, 80784942, 88226007, 21849873, 29159007, 22698199,
    64019323, 34771961, 32155593, 72961758, 75391157, 64441592, 20851987, 65069070, 37985701, 74806397,
    97074201, 18993851, 7954269, 27206140, 2653983, 59804214, 60549127, 79005365, 56848510, 76206364,
    1177768, 77433749, 12947086, 91835654, 25387101, 29632491, 93957257, 12178002, 18641325, 4234721,
    13370161, 56662427, 77725049, 1288931, 45511919, 19386996, 51064125, 48505617, 40108691, 74010258,
    57434631, 85432529, 71053818, 34316173, 45319574, 90592229, 44496794, 78211811, 71482070, 26153883,
    22734240, 12421402, 55083919, 83589288, 3188581, 21936213, 59450503, 88015521, 8485987, 72644423,
    89492168, 49234277, 71109490, 38880749, 1322935, 74539696, 92406326, 2496301, 7281368, 45895862,
    36798945, 67208613, 66518066, 67205286, 98337024, 4184601, 63838680, 61234565, 15689068, 2522448,
    95542387, 19980655, 53783478, 30529143, 27750457, 8491885, 71164503, 75648202, 1677024, 17046570,
    3288481, 71905672, 45304891, 63928620, 22937627, 89535236, 55048244, 54841315, 67626417, 42930453,
    43278203, 13297668, 54298358, 39154218, 32609718, 77865677, 43073878, 73038995, 30139223, 15838492,
    53594357, 3465482, 17052999, 16828939, 23958528, 59888055, 99160205, 5749796, 13492317, 13067425,
    63431221, 41050647, 95977398, 63744418, 50528885, 8214026, 61451873, 97355546, 46486384, 5550079,
    24431449, 72958089, 99869421, 90744338, 29058898, 69939355, 82213143, 44515874, 8188882, 50713794,
    67797236, 44280787, 34920659, 98818643, 34575897, 37869620, 76220337, 92012544, 33770331, 53629477,
    84901499, 98351232, 66475411, 50109103, 11149073, 2849526, 72006346, 65609504, 26370175, 45924423,
    47300514, 12160462, 65742008, 56326569, 95089258, 64631721, 17380173, 5419283, 16719959, 2704574,
    56306397, 34489744, 95787369, 53281923, 36941673, 17047563, 64228908, 49076289, 92303795, 32082550,
    65012633, 54213644, 80348352, 49102542, 42853952, 40778244, 1288789, 40232302, 58985814, 65447869,
    93028240, 1395930, 74426007, 80911411, 11755764, 11190353, 95380974, 80001701, 43647790, 28926692,
    77316273, 4204400, 53441274, 71932709, 28274039, 78882086, 11594120, 66246649, 91781899, 50177136,
    18677316, 28715016, 79725417, 13236619, 58783607, 90326557, 75174419, 23257675, 5288804, 40728210,
    44212628, 16804772, 98103323, 64564694, 98449643, 97022709, 41571706, 78746215, 84142874, 95880688,
    21142671, 60689878, 85448303, 23745686, 24047607, 33344398, 66111415, 45852350, 90531675, 14252449,
    7918985, 65700727, 68102624, 60338186, 20124340, 67088257, 40701095, 48039149, 51794233, 57739092,
    98885916, 22686338, 3278581, 52247993, 16271934, 54949096, 69438306, 58370945, 32363874, 59928618,
    19438875, 80453159, 4187037, 20173086, 46520854, 84729433, 40418981, 95804311, 24995836, 35813073,
    61423800, 22557883, 55604257, 7050177, 85385701, 22628869, 87553724, 69850701, 26543552, 34935762,
    4630427, 42811152, 23680756, 6838755, 16762571, 5290428, 12576151, 95612845, 48873712, 74127065,
    95216411, 43535805, 43164075, 99967097, 56123387, 95984883, 17048175, 92612168, 64559658, 90843138,
    94488717, 66819774, 3307282, 77056521, 49549516, 48902547, 83700273, 41720067, 28301692, 51505326,
    2283761, 28449660, 1583853, 78962097, 24150451, 33140500, 11088808, 95989821, 3287581, 99777251,
    21024506, 45535256, 3886317, 44409155, 65984762, 13810885, 39380125, 19214338, 95084509, 38974397,
    37321945, 54561436, 77181639, 12660399, 54156439, 39693646, 92790632, 30951333, 52592835, 47574347,
    57809607, 31874012, 32758284, 23151380, 95684948, 46528458, 1819833, 47263178, 66337890, 53804317,
    94149722, 62331274, 71604053, 79956445, 34559813, 97898378, 54033352, 13780748, 65864600, 55613158,
    26737183, 72976886, 51572606, 34184075, 76919374, 54818640, 43625002, 5481057, 63377703, 62878682,
    82000149, 80613424, 41660873, 81314656, 2407250, 61952663, 27368280, 16170202, 42587003, 70978151,
    73181865, 99831055, 84912152, 40875943, 55538881, 98855155, 36330098, 11764034, 46278668, 15577250,
    7054415, 80450734, 61595003, 32298067, 82022860, 73804299, 96667293, 83154241, 3288580, 82433010,
    70248553, 39680032, 62572158, 94013274, 31580314, 11099541, 50571358, 18933078, 13763634, 66190703,
    69869055, 92668320, 50845140, 49799882, 76649196, 55211268, 68897827, 62725173, 16741896, 42667891,
    92533112, 10941456, 70963044, 58343070, 49381200, 84189001, 44586035, 65888428, 73554148, 13800932,
    1791842, 31356380, 18803441, 70532338, 84892532, 69072756, 28361415, 71325639, 33568550, 4170649,
    76088993, 8898792, 20714020, 5563946, 8302403, 43111177, 46584669, 74683395, 12021831, 16377105,
    94692290, 61180300, 99785465, 93063093, 44500272, 3878392, 17309679, 91907164, 51263768, 39322041,
    62344440, 39635438, 15408959, 84066173, 93033583, 46134413, 99192317, 81084677, 33824793, 51447254

]
taxa_aprendizagem = 0.01
bias = -2  # Valor fixo do bias

# Treinamento
total_iteracoes, pesos_finais, bias_final, soma_erros, soma_erros_quadrado = treino_perceptron(numbers, taxa_aprendizagem,
                                                                                               bias)

# Exibir resultados no terminal
print(f"Total de iterações: {total_iteracoes}")
print(f"k: {taxa_aprendizagem}")
for i, peso in enumerate(pesos_finais):
    print(f"X{i+1} = {peso}")
print(f"W0: {bias_final}")
print(f"Soma dos erros na última iteração: {soma_erros}")
print(f"Soma quadrática dos erros na última iteração: {soma_erros_quadrado}")
